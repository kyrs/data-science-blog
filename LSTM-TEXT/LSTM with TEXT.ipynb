{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  following code is sample implementation of RNN using tf for Text\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "original code link https://github.com/campdav/text-rnn-tensorflow/blob/master/Train_RNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import time\n",
    "import os\n",
    "\n",
    "from six.moves import cPickle\n",
    "\n",
    "\n",
    "from newModel import  Model\n",
    "import codecs\n",
    "import collections\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'data/alice' ## defining the directory of the dataset\n",
    "inputEncoding = None ## https://docs.python.org/3/library/codecs.html#standard-encodings\n",
    "\n",
    "logDir = \"./logDir\"\n",
    "saveDir = \"./dataSave\"\n",
    "rnnSize = 256   # size of the rnn \n",
    "noLayers = 2   # no of layers in RNN\n",
    "model = \"lstm\" # lstm model\n",
    "batchSize= 20  # size of batch\n",
    "seqLength= 25  # length of the sequence\n",
    "noEpoch =25 # epoch to save\n",
    "saveEvery = 1000 # frequency of saving\n",
    "gradClip =  .5\n",
    "learningRate = 0.002\n",
    "decayRate = 0.95\n",
    "initFrom = None\n",
    "gpuMem = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loding the data into the memry\n",
    "inputFile = os.path.join(dataDir,'input.txt')\n",
    "vocabFile = os.path.join(dataDir,'vocab.pkl')\n",
    "tensorFile =os.path.join(dataDir,'tensor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(inputFile,\"r\",encoding=None) as f:\n",
    "    data = f.read() ## this function will read the full sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "## now we are going to split the data based on spaces. NOTE:- there is no split which is happening based on \n",
    "##. or some other punctuation\n",
    "\n",
    "xText = data.split()\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab List : 6019\n"
     ]
    }
   ],
   "source": [
    "wordCount = collections.Counter(xText)\n",
    "wordName = [x[0] for x in  wordCount.most_common()]\n",
    "wordName = list(sorted(wordName))\n",
    "\n",
    "vocab = {x:i for  i,x in enumerate(wordName)}\n",
    "words = [x[0] for x in  wordCount.most_common()]\n",
    "print(\"vocab List :\", len(words))\n",
    "\n",
    "with open(vocabFile, 'wb') as f:\n",
    "    cPickle.dump((words), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor is:[1066  786  539 ... 1330 3953 2481]\n",
      "It's shape: (29465,)\n"
     ]
    }
   ],
   "source": [
    "tensor = np.array(list(map(vocab.get, xText)))\n",
    "\n",
    "# Save the data to data.npy\n",
    "np.save(tensorFile, tensor)\n",
    "\n",
    "print('tensor is:' + str(tensor))\n",
    "print(\"It's shape: \" + str(np.shape(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the no of batches tp be processed\n",
    "numBatch = int(tensor.size/(seqLength*batchSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000\n"
     ]
    }
   ],
   "source": [
    "## data to be processed\n",
    "tensor = tensor[:numBatch*batchSize*seqLength]\n",
    "print (tensor.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xData = tensor\n",
    "yData = np.copy(tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have to set-up correctely the targets (ydata).\n",
    "\n",
    "in our exemple, we want to predict the next words of a sentence, so ydata is a shift by one word from xdata. In order to have a ydata with the same shape, we copy the first component of xdata to the last one of ydata.\n",
    "\n",
    "Dumb example: if the complete xdata is: \"the quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "xdata = [the, quick, brown, fox, jumps, over, the, lazy, dog]\n",
    "ydata = [quick, brown, fox, jumps, over, the, lazy, dog, the]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yData[:-1] = xData[1:] \n",
    "yData[-1] = xData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1066  786  539 ... 3405 5729 1474]\n",
      "[ 786  539  530 ... 5729 1474 1066]\n"
     ]
    }
   ],
   "source": [
    "print(xData)\n",
    "print(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 25)\n",
      "(20, 25)\n"
     ]
    }
   ],
   "source": [
    "# print (xData.shape)\n",
    "newData = xData.reshape(batchSize,-1)\n",
    "newLabel = yData.reshape(batchSize, -1)\n",
    "xBatchTrain = np.split(newData,numBatch,1)\n",
    "yBatchTrain = np.split(newLabel,numBatch,1)\n",
    "\n",
    "print(xBatchTrain[0].shape)\n",
    "print(yBatchTrain[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0 ## batch pointer\n",
    "with open(os.path.join(dataDir,\"word_vocab.pkl\"),\"wb\") as f:\n",
    "    cPickle.dump((words,vocab),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Model(dataDir=dataDir,inputEncoding=inputEncoding,logDir=logDir,saveDir=saveDir,rnnSize=rnnSize,numLayers=noLayers,model=model,batchSize=batchSize,seqLength=seqLength,noEpoch=noEpoch,\n",
    "\t\t\t\tsaveAfterEvery=saveEvery,gradClip=gradClip,learningRate=learningRate,decayRate=decayRate,gpuMemory=gpuMem,initFrom=initFrom,vocabSize=vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "trainWriter = tf.summary.FileWriter(logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpuMem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(arg):\n",
    "  arg = tf.convert_to_tensor(arg, dtype=tf.int64)\n",
    "  print (arg.shape)\n",
    "  return arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model in session environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1450 (epoch 0), train_loss = 8.702, time/batch = 0.425\n",
      "model saved to ./dataSave\\model_test.ckpt\n",
      "20/1450 (epoch 0), train_loss = 7.846, time/batch = 0.111\n",
      "40/1450 (epoch 0), train_loss = 7.269, time/batch = 0.106\n",
      "60/1450 (epoch 1), train_loss = 7.314, time/batch = 0.104\n",
      "80/1450 (epoch 1), train_loss = 6.969, time/batch = 0.105\n",
      "100/1450 (epoch 1), train_loss = 6.853, time/batch = 0.119\n",
      "120/1450 (epoch 2), train_loss = 7.114, time/batch = 0.110\n",
      "140/1450 (epoch 2), train_loss = 6.986, time/batch = 0.109\n",
      "160/1450 (epoch 2), train_loss = 6.836, time/batch = 0.105\n",
      "180/1450 (epoch 3), train_loss = 6.934, time/batch = 0.106\n",
      "200/1450 (epoch 3), train_loss = 6.952, time/batch = 0.112\n",
      "220/1450 (epoch 3), train_loss = 6.948, time/batch = 0.112\n",
      "240/1450 (epoch 4), train_loss = 6.852, time/batch = 0.116\n",
      "260/1450 (epoch 4), train_loss = 7.029, time/batch = 0.104\n",
      "280/1450 (epoch 4), train_loss = 6.810, time/batch = 0.101\n",
      "300/1450 (epoch 5), train_loss = 7.216, time/batch = 0.099\n",
      "320/1450 (epoch 5), train_loss = 6.828, time/batch = 0.104\n",
      "340/1450 (epoch 5), train_loss = 6.944, time/batch = 0.107\n",
      "360/1450 (epoch 6), train_loss = 6.823, time/batch = 0.109\n",
      "380/1450 (epoch 6), train_loss = 6.952, time/batch = 0.114\n",
      "400/1450 (epoch 6), train_loss = 6.859, time/batch = 0.104\n",
      "420/1450 (epoch 7), train_loss = 6.835, time/batch = 0.108\n",
      "440/1450 (epoch 7), train_loss = 6.825, time/batch = 0.103\n",
      "460/1450 (epoch 7), train_loss = 6.910, time/batch = 0.110\n",
      "480/1450 (epoch 8), train_loss = 6.761, time/batch = 0.119\n",
      "500/1450 (epoch 8), train_loss = 6.619, time/batch = 0.106\n",
      "520/1450 (epoch 8), train_loss = 6.698, time/batch = 0.100\n",
      "540/1450 (epoch 9), train_loss = 6.776, time/batch = 0.110\n",
      "560/1450 (epoch 9), train_loss = 6.809, time/batch = 0.108\n",
      "580/1450 (epoch 10), train_loss = 6.791, time/batch = 0.129\n",
      "600/1450 (epoch 10), train_loss = 7.041, time/batch = 0.108\n",
      "620/1450 (epoch 10), train_loss = 6.924, time/batch = 0.108\n",
      "640/1450 (epoch 11), train_loss = 6.816, time/batch = 0.145\n",
      "660/1450 (epoch 11), train_loss = 6.714, time/batch = 0.104\n",
      "680/1450 (epoch 11), train_loss = 6.645, time/batch = 0.112\n",
      "700/1450 (epoch 12), train_loss = 6.754, time/batch = 0.104\n",
      "720/1450 (epoch 12), train_loss = 6.724, time/batch = 0.114\n",
      "740/1450 (epoch 12), train_loss = 6.653, time/batch = 0.109\n",
      "760/1450 (epoch 13), train_loss = 6.658, time/batch = 0.105\n",
      "780/1450 (epoch 13), train_loss = 6.792, time/batch = 0.109\n",
      "800/1450 (epoch 13), train_loss = 6.816, time/batch = 0.114\n",
      "820/1450 (epoch 14), train_loss = 6.664, time/batch = 0.108\n",
      "840/1450 (epoch 14), train_loss = 6.873, time/batch = 0.107\n",
      "860/1450 (epoch 14), train_loss = 6.676, time/batch = 0.099\n",
      "880/1450 (epoch 15), train_loss = 6.960, time/batch = 0.114\n",
      "900/1450 (epoch 15), train_loss = 6.657, time/batch = 0.097\n",
      "920/1450 (epoch 15), train_loss = 6.770, time/batch = 0.103\n",
      "940/1450 (epoch 16), train_loss = 6.631, time/batch = 0.108\n",
      "960/1450 (epoch 16), train_loss = 6.785, time/batch = 0.103\n",
      "980/1450 (epoch 16), train_loss = 6.705, time/batch = 0.112\n",
      "1000/1450 (epoch 17), train_loss = 6.649, time/batch = 0.112\n",
      "model saved to ./dataSave\\model_test.ckpt\n",
      "1020/1450 (epoch 17), train_loss = 6.662, time/batch = 0.118\n",
      "1040/1450 (epoch 17), train_loss = 6.786, time/batch = 0.115\n",
      "1060/1450 (epoch 18), train_loss = 6.583, time/batch = 0.112\n",
      "1080/1450 (epoch 18), train_loss = 6.506, time/batch = 0.106\n",
      "1100/1450 (epoch 18), train_loss = 6.556, time/batch = 0.106\n",
      "1120/1450 (epoch 19), train_loss = 6.633, time/batch = 0.112\n",
      "1140/1450 (epoch 19), train_loss = 6.694, time/batch = 0.108\n",
      "1160/1450 (epoch 20), train_loss = 6.593, time/batch = 0.163\n",
      "1180/1450 (epoch 20), train_loss = 6.715, time/batch = 0.108\n",
      "1200/1450 (epoch 20), train_loss = 6.557, time/batch = 0.110\n",
      "1220/1450 (epoch 21), train_loss = 6.732, time/batch = 0.111\n",
      "1240/1450 (epoch 21), train_loss = 6.531, time/batch = 0.120\n",
      "1260/1450 (epoch 21), train_loss = 6.486, time/batch = 0.102\n",
      "1280/1450 (epoch 22), train_loss = 6.649, time/batch = 0.118\n",
      "1300/1450 (epoch 22), train_loss = 6.573, time/batch = 0.115\n",
      "1320/1450 (epoch 22), train_loss = 6.490, time/batch = 0.112\n",
      "1340/1450 (epoch 23), train_loss = 6.525, time/batch = 0.109\n",
      "1360/1450 (epoch 23), train_loss = 6.681, time/batch = 0.113\n",
      "1380/1450 (epoch 23), train_loss = 6.682, time/batch = 0.106\n",
      "1400/1450 (epoch 24), train_loss = 6.489, time/batch = 0.102\n",
      "1420/1450 (epoch 24), train_loss = 6.666, time/batch = 0.105\n",
      "1440/1450 (epoch 24), train_loss = 6.493, time/batch = 0.116\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    ## add the graph in log dir\n",
    "    trainWriter.add_graph(sess.graph)\n",
    "    \n",
    "    ## initialize all the global variable\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    ## making a saver file to save all the variable\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    \n",
    "    ## creating a for loop itereating the epoch \n",
    "    \n",
    "    for e in range(model.epochPointer.eval(),noEpoch):\n",
    "        \n",
    "        ## initializing the lr value of the model\n",
    "        sess.run(tf.assign(model.lr,learningRate*(decayRate)**e))\n",
    "        \n",
    "        ## definig the initial state of the system\n",
    "        state = sess.run(model.initialState)\n",
    "        \n",
    "        ## pointer for the batch\n",
    "        pointer = 0\n",
    "        \n",
    "        ## pointer over the speed \n",
    "        speed =0 \n",
    "        if initFrom is None:\n",
    "            ## initFrom function is none\n",
    "            assignOp = model.epochPointer.assign(e)\n",
    "            sess.run(assignOp)\n",
    "        else:\n",
    "            assignOp = model.batchPointer.eval()\n",
    "            initFrom = None\n",
    "    \n",
    "    \n",
    "        ## for each epoch, for loop to interate over each batch (b)\n",
    "        for b in range(pointer,numBatch):\n",
    "            start = time.time()\n",
    "            x, y = xBatchTrain[pointer],yBatchTrain[pointer]\n",
    "            pointer+=1\n",
    "            \n",
    "            feed = {model.inputData:x,model.targets:y,model.initialState : state,model.batchTime : speed}\n",
    "            print \n",
    "            summary,trainLoss,state,_,_ = sess.run([merged,model.cost,model.finalState,model.trainOp,model.incBatchPointerOp],feed)\n",
    "            \n",
    "            ## adding the summary writer  with the globalstep\n",
    "            trainWriter.add_summary(summary,8*numBatch+b)\n",
    "            \n",
    "            speed = time.time()-start\n",
    "            if (e * numBatch + b) % batchSize == 0:\n",
    "                print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                    .format(e * numBatch + b,\n",
    "                            noEpoch * numBatch,\n",
    "                            e, trainLoss, speed))\n",
    "\n",
    "                #save model:\n",
    "                if (e * numBatch + b) % saveEvery == 0 \\\n",
    "                        or (e==noEpoch-1 and b == numBatch-1): # save for the last result\n",
    "                    #define the path to the model\n",
    "                    checkpoint_path = os.path.join(saveDir, 'model_test.ckpt')\n",
    "                    #save the model, woth increment ()\n",
    "                    saver.save(sess, checkpoint_path, global_step = e * numBatch + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "        \n",
    "        #close the session\n",
    "    trainWriter.close()\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  code for testing the system using basic words - second phase (run separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import time \n",
    "import os\n",
    "from six.moves import cPickle\n",
    "\n",
    "from newModel import  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir = \"./dataSave\" ## directory to load data from \n",
    "\n",
    "n =200 ## no of words to sample from\n",
    "\n",
    "prime = \"Alice\"\n",
    "\n",
    "sample = 3 #0 to use max at each timestep, 1 to sample at each timestep, 2 to sample on spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputEncoding = None ## https://docs.python.org/3/library/codecs.html#standard-encodings\n",
    "\n",
    "dataDir = 'data/alice' ## defining the directory of the dataset\n",
    "logDir = \"./logDir\"\n",
    "saveDir = \"./dataSave\"\n",
    "rnnSize = 256   # size of the rnn \n",
    "noLayers = 2   # no of layers in RNN\n",
    "model = \"lstm\" # lstm model\n",
    "batchSize= 20  # size of batch\n",
    "seqLength= 25  # length of the sequence\n",
    "noEpoch =25 # epoch to save\n",
    "saveEvery = 1000 # frequency of saving\n",
    "gradClip =  .5\n",
    "learningRate = 0.002\n",
    "decayRate = 0.95\n",
    "initFrom = None\n",
    "gpuMem = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataDir, 'word_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Model(dataDir=dataDir,inputEncoding=inputEncoding,logDir=logDir,saveDir=saveDir,rnnSize=rnnSize,numLayers=noLayers,model=model,batchSize=batchSize,seqLength=seqLength,noEpoch=noEpoch,\n",
    "\t\t\t\tsaveAfterEvery=saveEvery,gradClip=gradClip,learningRate=learningRate,decayRate=decayRate,gpuMemory=gpuMem,initFrom=initFrom,vocabSize=vocabSize,infer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./dataSave\\model_test.ckpt-1000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    #then we define the Saver to retrieve the model\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    #we retrieve the checkpoint of the stored model:\n",
    "    ckpt = tf.train.get_checkpoint_state(saveDir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        #we restore the model\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        #we create the results\n",
    "        results = model.sample(sess, words, vocab, n, prime, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice dodged distance--but clever newspapers, strength, clever clever him--How whiting!' leaves please, encouraged started continued Beautiful, else's gravely. Time, larger: beautifully creation clever clever About paragraphs person silence, Stretching, them, altogether jogged Rabbit. Rabbit. encouraged NO quiet dream. This beauti--FUL Always to, brain; clever surprise encouraged Footman, beautifully beautifully (801) Northumbria--\"' else's business!' ear. shall!' ring, '--it get\" writing hadn't particularly held eagerly right ink, described appearing Hart, ink, choke North soldier eagerly creep dull them--and provisions. whiting!' Derision.' reach Him, whiting!' remarks,' thousand experiment beautifully TRADEMARK any) officer shower brought clever compliance. dream. because hot-tempered,' then--always doubtful body clever whiting!' this), FROM DISTRIBUTE previous me! might clever dream. Rabbit's--'Pat! way. frighten course--\"I 90 Any with; head!\"' eagerly low. furiously, clever glass; splashing tumbling happened, well?' before.' surprise 'Wow! sobs TO sneeze, English. guilt,' break else's States, Prizes!' pale, Melan asking, eagerly beautifully TRADEMARK clever eagerly them--and brain; paws! surprise clever 'Sh! knocked. walks away. crocodile break clever game,' clever '--it types outside,' clever received push finger; eagerly clever dream. quiet clever saucer Cat: LEAVE pinch jar kissed incomplete, beautifully particular--Here, Dinah!' MYSELF, dream. because certain types over!' clever him--How larger: Duchess! About beautifully clever brought living clever teaching clever Of\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
